# SPARCS Multi-Agent Evaluation Script

This repository provides a command-line evaluation script for **multi-agent reasoning with configurable communication structures**.  
It supports multiple benchmark datasets (`gsm8k`, `aqua`, `mmlu`) and evaluates multi-agent systems with either **fixed** or **LLM-generated** communication graphs.

The script is designed to work with the **SPARCS** framework and supports dynamic graph structures generated by an LLM under DAG constraints.

---

## Features

- Multi-agent reasoning with configurable agent numbers and roles
- Supported datasets:
  - `gsm8k`
  - `aqua`
  - `mmlu`
- Two communication structure modes:
  - **Fixed structures** (FullConnected, Chain, Star, Layered, Random, etc.)
  - **LLM-generated structures** (DAG, task-dependent)
- Multi-round agent interaction
- Automatic accuracy tracking and incremental result saving
- JSON output with per-sample prediction details

---

## Dependencies

Recommended Python version: **Python 3.9+**

Main external dependencies:

```bash
pip install torch numpy
```

Internal project dependencies (must exist under `project_root`):

- `SPARCS.graph.graph.Graph`
- `SPARCS.tools.reader.readers.JSONLReader`
- `SPARCS.utils.globals.Time`
- `SPARCS.llm.gpt_chat.GPTChat`
- `datasets/gsm8k_dataset.py`
- `datasets/aqua_dataset.py`
- `datasets/mmlu_dataset.py`

> ⚠️ If `GPTChat` requires API keys or external LLM access (e.g., OpenAI), configure them in `SPARCS/llm/gpt_chat.py`.

---

## Recommended Project Structure

```
project_root/
├─ SPARCS/
├─ aqua/
├─ datasets/
├─ experiments/
│  └─ run-sync-1.py          # main evaluation script (your entry point)
├─ result/
├─ rl/
├─ test.py
├─ utils_rl.py
└─ README.md
```

> Make sure to run scripts (e.g., `run-sync-1.py`) from a subdirectory like `experiments/`, as the root is automatically added to `sys.path`.

---

## Quick Start

### 1. LLM-Generated Communication Structure (Your Example)

```bash
python experiments/run-sync-1.py \
  --dataset_json "aqua/test.jsonl" \
  --domain "aqua" \
  --agent_names Planner MathSolver \
  --agent_nums 2 2 \
  --decision_method FinalRefer \
  --batch_size 1 \
  --structure_generator llm \
  --structure_llm_name qwen2_lora_sft \
  --llm_name meta-llama/llama-3-8b-instruct
```

### 2. Fixed Communication Structure (Example)

```bash
python experiments/run-sync-1.py \
  --dataset_json datasets/gsm8k/test.jsonl \
  --domain gsm8k \
  --structure_generator fixed \
  --fixed_mode FullConnected \
  --agent_names MathSolver \
  --agent_nums 4 \
  --llm_name gpt-3.5-turbo \
  --num_rounds 1 \
  --batch_size 4
```

---

## Command-Line Arguments

| Argument | Type | Default | Description |
|---|---|---|---|
| `--dataset_json` | str | - | Path to dataset (JSONL format) |
| `--result_file` | str | None | Optional output file path |
| `--domain` | str | - | Dataset domain: `gsm8k`, `aqua`, or `mmlu` |
| `--llm_name` | str | `gpt-3.5-turbo` | Model used by reasoning agents |
| `--structure_llm_name` | str | None | Model used to generate communication graph |
| `--agent_names` | list[str] | `['MathSolver']` | Agent role names |
| `--agent_nums` | list[int] | `[4]` | Number of agents per role |
| `--decision_method` | str | `FinalRefer` | Aggregation strategy |
| `--num_rounds` | int | 1 | Number of communication rounds |
| `--structure_generator` | str | `fixed` | `fixed` or `llm` |
| `--fixed_mode` | str | `FullConnected` | Predefined topology |
| `--batch_size` | int | 4 | Save results every N samples |

> ⚠️ `agent_names` and `agent_nums` must match in length.

---

## Fixed Communication Structures

Supported `--fixed_mode` options:

- `FullConnected` – fully connected (default)
- `Chain` – linear: 0 → 1 → ... → N-1
- `Star` – one-to-all (0 → all)
- `Layered` – layered structure
- `Random` – random DAG
- `DirectAnswer` / `Debate` – no communication

---

## LLM-Generated Structures

If `--structure_generator` is set to `llm`, the script uses an LLM to dynamically generate a DAG structure for agents:

- Input: task description + agent list
- Output: communication matrix (no self-loops, must be DAG)
- Constraints:
  - One final output node (sink)
  - Evaluation-only agents (like FinalRefer) cannot be final node

If structure generation fails, the last valid structure is reused.

---

## Output Format

Each sample is saved in a JSON object like:

```json
{
  "Task_ID": 0,
  "Structure_Generator": "llm",
  "Question": "...",
  "True Answer": "...",
  "Response": "...",
  "Predicted Answer": "...",
  "Solved": true,
  "Overall Accuracy": "0.8000"
}
```

---

## Tips

- Frequently save results (`--batch_size=1` is safest for debugging)
- Check that your dataset (`test.jsonl`) matches the required format
- Ensure that custom models like `qwen2_lora_sft` are loadable inside the codebase or handled by the inference module
- To debug structure generation or agent reasoning, inspect intermediate logs or insert print statements in `SPARCS/llm/gpt_chat.py`

## License

Specify your project license here (e.g., MIT, Apache-2.0, or private).

---
