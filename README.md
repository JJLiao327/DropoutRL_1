# SPARCS Multi-Agent Evaluation Script

This repository provides a command-line evaluation script for **multi-agent reasoning with configurable communication structures**.  
It supports multiple benchmark datasets (`gsm8k`, `aqua`, `mmlu`) and evaluates multi-agent systems with either **fixed** or **LLM-generated** communication graphs.

The script is designed to work with the **SPARCS** framework and supports dynamic graph structures generated by an LLM under DAG constraints.

---

## Features

- Multi-agent reasoning with configurable agent numbers and roles
- Supported datasets:
  - `gsm8k`
  - `aqua`
  - `mmlu`
- Two communication structure modes:
  - **Fixed structures** (FullConnected, Chain, Star, Layered, Random, etc.)
  - **LLM-generated structures** (DAG, task-dependent)
- Multi-round agent interaction
- Automatic accuracy tracking and incremental result saving
- JSON output with per-sample prediction details

---

## Dependencies

Recommended Python version: **Python 3.9+**

Main external dependencies:

```bash
pip install torch numpy
```

Internal project dependencies (must be available under `project_root`):

- `SPARCS.graph.graph.Graph`
- `SPARCS.tools.reader.readers.JSONLReader`
- `SPARCS.utils.globals.Time`
- `SPARCS.llm.gpt_chat.GPTChat`
- `datasets/gsm8k_dataset.py`
- `datasets/aqua_dataset.py`
- `datasets/mmlu_dataset.py`

> ⚠️ If `GPTChat` requires API keys (e.g., OpenAI), configure them according to your implementation in `SPARCS/llm/gpt_chat.py`.

---

## Recommended Project Structure

```
project_root/
├─ scripts/
│  └─ run_eval.py            # main evaluation script
├─ SPARCS/
│  ├─ graph/
│  ├─ llm/
│  ├─ tools/
│  └─ utils/
├─ datasets/
│  ├─ gsm8k/
│  │  └─ test.jsonl
│  ├─ gsm8k_dataset.py
│  ├─ aqua_dataset.py
│  └─ mmlu_dataset.py
└─ result/
   └─ <domain>/
      └─ *.json
```

The script automatically adds `project_root` to `sys.path`, so it should be run from a subdirectory of the project root (e.g., `scripts/`).

---

## Quick Start

### 1. Fixed Communication Structure (Default)

```bash
python scripts/run_eval.py \
  --domain gsm8k \
  --dataset_json datasets/gsm8k/test.jsonl \
  --structure_generator fixed \
  --fixed_mode FullConnected \
  --agent_names MathSolver \
  --agent_nums 4 \
  --llm_name gpt-3.5-turbo \
  --num_rounds 1 \
  --batch_size 4
```

### 2. LLM-Generated Communication Structure

```bash
python scripts/run_eval.py \
  --domain gsm8k \
  --dataset_json datasets/gsm8k/test.jsonl \
  --structure_generator llm \
  --structure_llm_name gpt-4o-mini \
  --agent_names MathSolver \
  --agent_nums 4 \
  --llm_name gpt-3.5-turbo \
  --num_rounds 1 \
  --batch_size 4
```

- `--structure_llm_name`: model used to generate the communication graph
- `--llm_name`: model used by agents during reasoning

If `--structure_llm_name` is not provided, it defaults to `--llm_name`.

---

## Command-Line Arguments

| Argument | Type | Default | Description |
|---|---|---|---|
| `--dataset_json` | str | `datasets/gsm8k/test.jsonl` | Path to input dataset (JSONL) |
| `--result_file` | str | None | Output file path (auto-generated if not set) |
| `--domain` | str | `gsm8k` | Dataset domain: `gsm8k`, `aqua`, or `mmlu` |
| `--llm_name` | str | `gpt-3.5-turbo` | LLM used for agent reasoning |
| `--structure_llm_name` | str | None | LLM used for structure generation |
| `--agent_names` | list[str] | `['MathSolver']` | Agent role names |
| `--agent_nums` | list[int] | `[4]` | Number of agents per role |
| `--decision_method` | str | `FinalRefer` | Decision aggregation method |
| `--num_rounds` | int | 1 | Number of interaction rounds |
| `--structure_generator` | str | `fixed` | `fixed` or `llm` |
| `--fixed_mode` | str | `FullConnected` | Fixed graph topology |
| `--batch_size` | int | 4 | Save results every N samples |

> ⚠️ `agent_names` and `agent_nums` must have the same length.

---

## Fixed Communication Structures

Supported `--fixed_mode` options:

- `FullConnected` – fully connected graph (default)
- `Chain` – linear chain: 0 → 1 → 2 → ... → N-1
- `Star` – star topology: 0 → all others
- `Layered` – first half of agents connect to second half
- `Random` – random directed edges (no self-loops)
- `DirectAnswer` / `Debate` – no inter-agent communication

---

## LLM-Generated Structures (`structure_generator=llm`)

When enabled:

- The script sends the task description and agent list to an LLM
- The LLM returns a JSON object specifying:
  - Selected agent indices
  - A communication matrix (DAG)
- The matrix is expanded into a full `N × N` adjacency matrix

### Constraints enforced in the prompt

- The communication graph must be a **Directed Acyclic Graph (DAG)**
- No self-loops
- A clear final agent (sink node) must exist
- Evaluation-only agents (e.g., `FinalRefer`, `Checker`) must not be used as final output nodes

If structure generation fails, the script automatically falls back to the previous valid structure.

---

## Output Format

Results are saved as a JSON list. Each entry has the following structure:

```json
{
  "Task_ID": 0,
  "Structure_Generator": "fixed",
  "Question": "...",
  "True Answer": "...",
  "Response": "...",
  "Predicted Answer": "...",
  "Solved": true,
  "Overall Accuracy": "0.7500"
}
```

- `Overall Accuracy` is the cumulative accuracy up to the current sample
- Results are written to disk every `batch_size` samples

---

## Notes & Tips

- For long runs, frequent saving helps prevent data loss if the process is interrupted
- Ensure dataset preprocessing functions (`*_data_process`) match your JSONL schema
- For `mmlu`, each record must include at least:
  - `task`
  - `answer`

---

## License

Specify your project license here (e.g., MIT, Apache-2.0, or private).

---
