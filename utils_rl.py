"""
utils_rl.py
Utils for reinforcement learning in multi-agent communication environments.

é€‚ç”¨äºå¼ºåŒ–å­¦ä¹ é€šä¿¡ç»“æ„ä¼˜åŒ–é¡¹ç›®
Author: ChatGPT + Tianzhe
Updated: Apr-2025
"""

from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
import numpy as np
import networkx as nx


# =============================
# ğŸ¯ é€šä¿¡å›¾å¥–åŠ±ç»“æ„
# =============================

REWARD_WEIGHTS = {
    "task_perf": 1.0,            # ä»»åŠ¡æ€§èƒ½ï¼ˆå‡†ç¡®æ€§ã€å¾—åˆ†ç­‰ï¼‰
    "comm_cost": -0.05,          # é€šä¿¡å¼€é”€ï¼ˆèŠ‚ç‚¹æ•°ã€è¾¹æ•°ã€tokenæ•°ï¼‰
    "graph_diversity": 0.1,      # å›¾ç»“æ„å¤šæ ·æ€§é¼“åŠ±é¡¹
    "dropout_efficiency": 0.1,   # Dropoutæ˜¯å¦ç²¾å‡†ï¼ˆè¸¢æ‰æ— ç”¨Agentï¼‰
}


@dataclass
class StepResult:
    reward: float
    info: Dict

    def __iter__(self):
        return iter((self.reward, self.info))


def compute_task_performance(task_metric: float) -> float:
    """ä»»åŠ¡å®Œæˆåº¦å¾—åˆ†ï¼Œæ ‡å‡†åŒ–åˆ°[0, 1]"""
    return np.clip(task_metric, 0.0, 1.0)


def compute_comm_cost(graph: nx.Graph, max_nodes: int, max_edges: int) -> float:
    """é€šä¿¡å›¾çš„æˆæœ¬ï¼ˆèŠ‚ç‚¹ + è¾¹ï¼‰å½’ä¸€åŒ–"""
    node_penalty = len(graph.nodes) / max_nodes
    edge_penalty = len(graph.edges) / max_edges
    return node_penalty + edge_penalty


def compute_graph_diversity(graph: nx.Graph) -> float:
    """å›¾ç»“æ„å¤šæ ·æ€§åº¦é‡ï¼Œå¯æ›¿æ¢ä¸ºä¿¡æ¯ç†µ / Degree variance"""
    degrees = [d for _, d in graph.degree()]
    if len(degrees) <= 1:
        return 0.0
    return np.std(degrees) / (np.mean(degrees) + 1e-5)


def compute_dropout_efficiency(dropout_mask: List[int], useless_agent_ids: List[int]) -> float:
    """Dropoutæ˜¯å¦ç²¾å‡†ï¼šæ˜¯å¦è¸¢æ‰äº†çœŸæ­£æ²¡ç”¨çš„Agent"""
    correct_drops = sum([1 for i in useless_agent_ids if dropout_mask[i] == 0])
    return correct_drops / (len(useless_agent_ids) + 1e-5)


# =============================
# ğŸ§  æ€»å¥–åŠ±å‡½æ•°
# =============================

def compute_total_reward(task_metric: float,
                         graph: nx.Graph,
                         dropout_mask: List[int],
                         useless_agent_ids: List[int],
                         max_nodes: int,
                         max_edges: int) -> StepResult:
    """
    ç»¼åˆå¥–åŠ±è®¡ç®—å‡½æ•°ï¼ˆç”¨äºPPOç¯å¢ƒä¸­ï¼‰
    """

    task_score = compute_task_performance(task_metric)
    comm_cost = compute_comm_cost(graph, max_nodes, max_edges)
    diversity = compute_graph_diversity(graph)
    dropout_eff = compute_dropout_efficiency(dropout_mask, useless_agent_ids)

    total_reward = (
        REWARD_WEIGHTS["task_perf"] * task_score +
        REWARD_WEIGHTS["comm_cost"] * comm_cost +
        REWARD_WEIGHTS["graph_diversity"] * diversity +
        REWARD_WEIGHTS["dropout_efficiency"] * dropout_eff
    )

    return StepResult(total_reward, {
        "task_score": task_score,
        "comm_cost": comm_cost,
        "diversity": diversity,
        "dropout_eff": dropout_eff
    })


# =============================
# ğŸ“¦ PPO Buffer Utility
# =============================

@dataclass
class Transition:
    state: Dict
    action: Dict
    reward: float
    next_state: Dict
    done: bool
    log_prob: Optional[float] = None
    value: Optional[float] = None
    advantage: Optional[float] = None
    return_: Optional[float] = None


class PPOBuffer:
    """ç”¨äºæ”¶é›†äº¤äº’è½¨è¿¹"""
    def __init__(self):
        self.buffer = []

    def store(self, transition: Transition):
        self.buffer.append(transition)

    def clear(self):
        self.buffer = []

    def get(self) -> List[Transition]:
        return self.buffer

    def compute_advantages(self, gamma: float = 0.99, lam: float = 0.95):
        """GAE Advantageè®¡ç®—"""
        rewards = [t.reward for t in self.buffer]
        values = [t.value for t in self.buffer]
        advantages = []
        gae = 0
        for t in reversed(range(len(rewards))):
            delta = rewards[t] + gamma * (values[t+1] if t + 1 < len(values) else 0) - values[t]
            gae = delta + gamma * lam * gae
            advantages.insert(0, gae)
        for i, adv in enumerate(advantages):
            self.buffer[i].advantage = adv
            self.buffer[i].return_ = self.buffer[i].advantage + self.buffer[i].value


# =============================
# ğŸ”§ é€šä¿¡ç»“æ„åˆ†æå·¥å…·
# =============================

def build_graph_from_mask(agent_mask: List[int], edge_mask: List[Tuple[int, int]]) -> nx.Graph:
    """æ ¹æ®åŠ¨ä½œè¾“å‡ºæ„å»ºå­å›¾"""
    G = nx.Graph()
    active_nodes = [i for i, keep in enumerate(agent_mask) if keep == 1]
    G.add_nodes_from(active_nodes)
    for (i, j) in edge_mask:
        if i in G.nodes and j in G.nodes:
            G.add_edge(i, j)
    return G


def summarize_graph_stats(graph: nx.Graph) -> Dict:
    """è¾“å‡ºå›¾çš„ç»“æ„æ€§ç»Ÿè®¡æŒ‡æ ‡"""
    return {
        "num_nodes": graph.number_of_nodes(),
        "num_edges": graph.number_of_edges(),
        "avg_degree": np.mean([d for _, d in graph.degree()]) if graph.number_of_nodes() > 0 else 0,
        "density": nx.density(graph) if graph.number_of_nodes() > 1 else 0
    }


# =============================
# âœ… Debug æµ‹è¯•ç”¨ä¾‹
# =============================

if __name__ == "__main__":
    import random

    # éšæœºç”Ÿæˆä¸€ä¸ªé€šä¿¡å­å›¾
    agent_mask = [1, 0, 1, 1, 0]  # æ€»å…±5ä¸ªAgentï¼Œåªæœ‰0/2/3ä¿ç•™
    edge_mask = [(0, 2), (2, 3), (0, 3), (1, 2)]  # åªæœ‰å…¶ä¸­å‡ ä¸ªæœ‰æ•ˆ

    G = build_graph_from_mask(agent_mask, edge_mask)

    # æ¨¡æ‹Ÿæƒ…å†µ
    task_metric = 0.85  # å‡è®¾ç³»ç»Ÿæ­£ç¡®å®Œæˆä»»åŠ¡
    useless_agents = [1, 4]  # å®é™…æ— ç”¨çš„Agent
    max_nodes, max_edges = 5, 10

    result = compute_total_reward(task_metric, G, agent_mask, useless_agents, max_nodes, max_edges)

    print(f"âœ… Total Reward: {result.reward:.4f}")
    print("ğŸ“Š Breakdown:", result.info)
    print("ğŸ“ˆ Graph Stats:", summarize_graph_stats(G))
